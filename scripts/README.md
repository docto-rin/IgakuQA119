# Script: compare_wrong_answers.py

## Purpose

This script compares the incorrect answers from two different Large Language Model (LLM) experiments run on the 119th Japanese Medical Licensing Examination (JMLE) dataset (IgakuQA119). It generates detailed comparison reports in CSV and/or Markdown formats, and optionally performs an analysis of the comparison using a specified LLM.

## Prerequisites

1.  **Python Environment:** Ensure you have the necessary Python packages installed. Running `uv sync` in the project root directory should install everything listed in `pyproject.toml`.
2.  **Experiment Results:** Before running this script, you must have already:
    *   Run `main.py` for the two experiments (`<exp1>` and `<exp2>`) you want to compare. This generates the answer JSON files in the `answers/` directory (e.g., `answers/119A_<exp1>.json`).
    *   Run `grade_answers.py` for both experiments. This generates the wrong answer CSV files in the `results/` directory (e.g., `results/119_<exp1>_wrong_answers.csv`).
3.  **Correct Answers:** The file `results/correct_answers.csv` must exist. It is generated by `scripts/prepro_utils/create_correct_answers_csv.py`.
4.  **API Keys (Optional):** If you plan to use the `--analyze_with_llm` option, ensure the corresponding API key for the specified LLM model is set in your `.env` file in the project root.

## Usage

Run the script from the project root directory using `uv run`:

```bash
uv run python scripts/compare_wrong_answers.py <exp1> <exp2> [options]
```

**Required Arguments:**

*   `<exp1>`: The identifier for the first experiment (e.g., `gemini-2.5-pro`). This corresponds to the `{EXP}` part in the result filenames.
*   `<exp2>`: The identifier for the second experiment (e.g., `CA-DSR1-DQ32B-JP`).

**Options:**

*   `--results_dir DIRECTORY`: Path to the directory containing the `_wrong_answers.csv` and `correct_answers.csv` files. (Default: `results`)
*   `--answers_dir DIRECTORY`: Path to the directory containing the answer detail JSON files (e.g., `119A_*.json`). (Default: `answers`)
*   `--output_dir DIRECTORY`: Path to the directory where the comparison output files will be saved. (Default: `results`)
*   `--output_format {csv,md,all}`: Output format for the comparison report.
    *   `csv`: Generate only the detailed CSV file.
    *   `md`: Generate only the Markdown report files (split by comparison type).
    *   `all`: Generate both CSV and Markdown files. (Default: `all`)
*   `--analyze_with_llm MODEL_KEY`: (Optional) Specify an LLM model key (e.g., `gemini-2.5-pro-exp-03-25`, `gpt-4o`) to perform a comparative analysis of the wrong answers. The analysis result will be saved as a separate Markdown file. Requires the corresponding API key to be set.

## Examples

**1. Basic Comparison (CSV and split Markdown reports):**

```bash
uv run python scripts/compare_wrong_answers.py gemini-2.5-pro CA-DSR1-DQ32B-JP
```

**2. Generate only the CSV report:**

```bash
uv run python scripts/compare_wrong_answers.py gpt-4o gemma-3 --output_format csv
```

**3. Generate split Markdown reports and perform LLM analysis using Gemini 2.5 Pro:**

```bash
uv run python scripts/compare_wrong_answers.py gemini-2.5-pro my_finetuned_model --output_format md --analyze_with_llm gemini-2.5-pro-exp-03-25
```

**4. Specify different input/output directories:**

```bash
uv run python scripts/compare_wrong_answers.py exp_A exp_B --results_dir path/to/results --answers_dir path/to/answers --output_dir path/to/output
```

## Output Files

Depending on the options used, the script will generate files in the specified `--output_dir` (default: `results`):

1.  **CSV Report (`comparison_119_<exp1>_vs_<exp2>.csv`):**
    *   Contains detailed information for each question where at least one model was wrong.
    *   Columns include: `question_number`, `comparison_type` (Both Wrong, <exp1> Wrong Only, <exp2> Wrong Only), `correct_answer`, `question_text`, `<exp1>_answer`, `<exp1>_explanation`, `<exp2>_answer`, `<exp2>_explanation`.

2.  **Markdown Reports (Split) (`comparison_119_<exp1>_vs_<exp2>_*.md`):**
    *   Generated if `output_format` is `md` or `all`.
    *   Separate files are created for each `comparison_type`:
        *   `..._Both_Wrong.md`
        *   `..._<exp1>_Wrong_Only.md`
        *   `..._<exp2>_Wrong_Only.md`
    *   Each file contains a summary, detailed breakdown per question (including answers and explanations), and the "Both Wrong" report includes a simple analysis of common wrong answer patterns for single-choice questions.

3.  **LLM Analysis Report (`analysis_119_<exp1>_vs_<exp2>_by_<MODEL_KEY>.md`):**
    *   Generated only if `--analyze_with_llm` is used.
    *   Contains the comparative analysis and insights generated by the specified LLM based on the Markdown reports.

## Notes

*   **LLM Analysis:** Using the `--analyze_with_llm` feature requires API calls, which may incur costs and take time. The quality of the analysis depends on the capability of the chosen LLM and the clarity of the generated reports. Very long reports might exceed the LLM's context window limit.
*   **Path Configuration:** The script assumes it's run from the project root directory (`IgakuQA119`). It relies on relative paths to find the `src` directory for importing `LLMSolver`. Ensure your execution context is correct.
